#!/usr/bin/env python3
"""
è³‡æ–™å¢é‡è…³æœ¬ï¼š
è®€å–åŸå§‹ HDF5 è³‡æ–™é›†ï¼Œå°è§¸è¦ºå½±åƒé€²è¡Œéš¨æ©Ÿå¢é‡ï¼Œä¸¦ç”Ÿæˆä¸€å€‹æ›´å¤§çš„æ–°è³‡æ–™é›†ã€‚
"""

import h5py
import numpy as np
import torch
import torchvision.transforms as T
from pathlib import Path
from tqdm import tqdm
import os

def get_augmentation_pipeline():
    """å®šç¾©æˆ‘å€‘çš„éš¨æ©Ÿå¢é‡æµç¨‹"""
    # é€™äº›å¢é‡æœƒä»¥ä¸€å®šçš„æ©Ÿç‡è¢«éš¨æ©Ÿæ‡‰ç”¨
    return T.Compose([
        T.RandomApply([
            T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05)
        ], p=0.8),
        T.RandomApply([
            T.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))
        ], p=0.5),
        T.RandomApply([
            T.ElasticTransform(alpha=35.0, sigma=4.0)
        ], p=0.5),
    ])

def process_episode(input_file, output_path, aug_pipeline):
    """è™•ç†å–®ä¸€ episode æª”æ¡ˆï¼Œæ‡‰ç”¨å¢é‡ä¸¦å„²å­˜"""
    with h5py.File(input_file, 'r') as hf_in:
        with h5py.File(output_path, 'w') as hf_out:
            # 1. è¤‡è£½æ‰€æœ‰éå½±åƒè³‡æ–™ (ä¾‹å¦‚æ©Ÿå™¨äººç‹€æ…‹)
            for key, item in hf_in.items():
                if not isinstance(item, h5py.Group):
                    hf_out.create_dataset(key, data=item[()])
            
            # 2. å°è§¸è¦ºå½±åƒé€²è¡Œå¢é‡
            if 'gelsight/gelsight' in hf_in:
                original_images_np = hf_in['gelsight/gelsight'][()]
                
                augmented_images = []
                for img_np in original_images_np:
                    # å°‡å½±åƒè½‰ç‚º tensor [C, H, W]
                    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1)
                    
                    # æ‡‰ç”¨å¢é‡ (åªåœ¨ uint8 ä¸Šæ“ä½œ)
                    augmented_tensor = aug_pipeline(img_tensor)
                    
                    # è½‰å› numpy [H, W, C]
                    augmented_np = augmented_tensor.permute(1, 2, 0).numpy()
                    augmented_images.append(augmented_np)
                
                # å„²å­˜å¢é‡å¾Œçš„å½±åƒ
                hf_out.create_dataset('gelsight/gelsight', data=np.array(augmented_images))
            
            # 3. (å¯é¸) æ‚¨ä¹Ÿå¯ä»¥å°å…¶ä»–ç›¸æ©Ÿå½±åƒåšåŒæ¨£çš„å¢é‡
            for camera in ['camera1', 'camera2']:
                dset_name = f'{camera}/{camera}'
                if dset_name in hf_in:
                     hf_out.create_dataset(dset_name, data=hf_in[dset_name][()])


def augment_dataset(input_dir, output_dir, augmentation_factor=5):
    """
    å°æ•´å€‹è³‡æ–™é›†é€²è¡Œå¢é‡

    Args:
        input_dir (str): åŸå§‹ HDF5 è³‡æ–™å¤¾è·¯å¾‘
        output_dir (str): å¢é‡å¾Œè³‡æ–™å„²å­˜çš„è·¯å¾‘
        augmentation_factor (int): å¢é‡å€æ•¸ï¼Œæ¯å€‹åŸå§‹æª”æ¡ˆæœƒç”Ÿæˆ N å€‹å¢é‡ç‰ˆæœ¬
    """
    input_path = Path(input_dir)
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å–å¾—åŸå§‹æª”æ¡ˆåˆ—è¡¨
    original_files = sorted(list(input_path.glob('*.h5')))
    if not original_files:
        print(f"âŒ éŒ¯èª¤ï¼šåœ¨ '{input_dir}' ä¸­æ‰¾ä¸åˆ°ä»»ä½• .h5 æª”æ¡ˆ")
        return
        
    print(f"ğŸ“‚ æ‰¾åˆ° {len(original_files)} å€‹åŸå§‹ episode æª”æ¡ˆã€‚")
    print(f"âš™ï¸ å¢é‡å€æ•¸ï¼š{augmentation_factor}x")
    print(f"ğŸ’¾ è¼¸å‡ºç›®éŒ„ï¼š{output_dir}")
    print("-" * 60)

    # å–å¾—å¢é‡æµç¨‹
    aug_pipeline = get_augmentation_pipeline()

    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    with tqdm(total=len(original_files) * augmentation_factor, desc="è³‡æ–™å¢é‡ä¸­") as pbar:
        # 1. å…ˆå°‡åŸå§‹æª”æ¡ˆè¤‡è£½ä¸€ä»½åˆ°æ–°ç›®éŒ„
        for original_file in original_files:
            new_path = output_path / original_file.name
            if not new_path.exists():
                os.link(original_file, new_path) # ä½¿ç”¨ç¡¬é€£çµï¼Œå¿«é€Ÿä¸”ä¸ä½”ç©ºé–“
            pbar.set_postfix_str(f"è¤‡è£½: {original_file.name}")
        
        # 2. ç”Ÿæˆå¢é‡æª”æ¡ˆ
        for i in range(augmentation_factor - 1): # -1 å› ç‚ºåŸå§‹æª”ç®—ä¸€ä»½
            for original_file in original_files:
                output_filename = f"{original_file.stem}_aug_{i+1}.h5"
                output_filepath = output_path / output_filename
                
                process_episode(original_file, output_filepath, aug_pipeline)
                pbar.update(1)
                pbar.set_postfix_str(f"ç”Ÿæˆ: {output_filename}")
    
    total_files = len(list(output_path.glob('*.h5')))
    print("-" * 60)
    print(f"âœ… è³‡æ–™å¢é‡å®Œæˆï¼")
    print(f"   ç¸½å…±ç”Ÿæˆäº† {total_files} å€‹ HDF5 æª”æ¡ˆã€‚")

if __name__ == "__main__":
    # --- åƒæ•¸è¨­å®š ---
    INPUT_DATA_DIR = './data/datasets/mango_hdf5_gelsight'
    OUTPUT_DATA_DIR = './data/datasets/mango_hdf5_augmented_30x'
    AUGMENTATION_FACTOR = 30  # æˆ‘å€‘å°‡è³‡æ–™æ“´å……åˆ° 30 å€
    
    augment_dataset(INPUT_DATA_DIR, OUTPUT_DATA_DIR, AUGMENTATION_FACTOR)