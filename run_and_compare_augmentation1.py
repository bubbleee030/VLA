# run_and_compare_augmentation.py (æœ€çµ‚é‡æ§‹ç‰ˆ)

import argparse
import yaml
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import shutil
import numpy as np
from pytorch_lightning import Trainer, seed_everything
from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor
from pytorch_lightning.loggers import CSVLogger
from residual_controller.bridge_model import ResidualController
from residual_controller.controller_dataset import ControllerDataModule

sns.set_style("whitegrid")

def train_single_model(config, data_dir, modality_name, suffix):
    """è¨“ç·´å–®ä¸€æ¨¡å‹ä¸¦å›å‚³å…¶æ—¥èªŒæª”æ¡ˆçš„è·¯å¾‘ã€‚"""
    # (æ­¤å‡½å¼èˆ‡æ‚¨ä¹‹å‰çš„ç‰ˆæœ¬å®Œå…¨ç›¸åŒï¼Œç„¡éœ€æ”¹å‹•)
    seed_everything(config['data']['random_seed'], workers=True)
    run_name = f"{modality_name}_{suffix}"
    output_dir = Path("outputs") / run_name
    log_dir_root = Path("logs/csv")
    
    if output_dir.exists():
        print(f"ğŸ§¹ Cleaning up old model directory: {output_dir}")
        shutil.rmtree(output_dir)
    log_dir = log_dir_root / run_name
    if log_dir.exists():
        print(f"ğŸ§¹ Cleaning up old log directory: {log_dir}")
        shutil.rmtree(log_dir)

    data_module = ControllerDataModule(
        h5_path_or_dir=data_dir,
        horizon=config['model']['horizon'],
        batch_size=config['training']['batch_size'],
        num_workers=config['data']['num_workers'],
        train_ratio=config['data']['train_ratio'],
        seed=config['data']['random_seed']
    )
    model = ResidualController(
        action_dim=config['model']['action_dim'],
        obs_dim=config['model']['obs_dim'],
        horizon=config['model']['horizon'],
        lr=config['training']['learning_rate'],
        tactile_feature_dim=config['model'].get('tactile_feature_dim', 126)
    )
    logger = CSVLogger(save_dir=log_dir_root, name=run_name, version=0)
    checkpoint_callback = ModelCheckpoint(
        dirpath=output_dir,
        filename='best-epoch={epoch}-val_loss={val_loss:.4f}',
        monitor='val_loss',
        mode='min',
        save_top_k=1,
        auto_insert_metric_name=False
    )
    lr_monitor = LearningRateMonitor(logging_interval='epoch')
    trainer = Trainer(
        max_epochs=config['training']['epochs'],
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        devices=1,
        logger=logger,
        callbacks=[checkpoint_callback, lr_monitor],
        log_every_n_steps=config['logging']['log_every_n_steps'],
        precision=config['training']['precision']
    )
    trainer.fit(model, datamodule=data_module)
    trainer.test(model, datamodule=data_module)
    
    log_file_path = Path(logger.log_dir) / "metrics.csv"
    print(f"âœ… Training complete for: {run_name}")
    print(f"ğŸ“Š Logs saved to: {log_file_path}\n")
    
    return log_file_path

def plot_learning_curves(ax, all_metrics, colors, markers):
    """åœ¨çµ¦å®šçš„ Matplotlib Axes ä¸Šç¹ªè£½å­¸ç¿’æ›²ç·š (Training/Validation Loss + Validation RÂ²)ã€‚"""
    # Validation RÂ² åŠ ä¸Šå¾Œæœ‰é»é†œ æ‰€ä»¥å…ˆè¨»è§£æ‰
    for name, data in all_metrics.items():
        color = colors.get(name)
        marker = markers.get(name)
        
        # è®€å–å®Œæ•´çš„ DataFrame
        df = data['df']
        
        # åœ¨ç¹ªåœ–å‰ï¼Œéæ¿¾æ‰åŒ…å« NaN çš„è¡Œ
        # é‡å° Validation Lossï¼Œåªå– 'epoch' å’Œ 'val_loss' éƒ½æœ‰å€¼çš„è¡Œ
        val_df = df[['epoch', 'val_loss']].dropna()
        
        # é‡å° Training Lossï¼Œåªå– 'epoch' å’Œ 'train_loss_epoch' éƒ½æœ‰å€¼çš„è¡Œ
        train_df = df[['epoch', 'train_loss_epoch']].dropna()
        
        # é‡å° Validation RÂ² Scoreï¼Œåªå– 'epoch' å’Œ 'val_r2' éƒ½æœ‰å€¼çš„è¡Œ
        # val_r2_df = df[['epoch', 'val_r2']].dropna()

        # ç¹ªè£½ Validation Loss
        ax.plot(val_df['epoch'], val_df['val_loss'], marker=marker, markersize=4, linestyle='-', color=color, label=f'Val Loss ({name})', linewidth=2)
        
        # ç¹ªè£½ Training Loss (ä½¿ç”¨æ›´æ·¡çš„é¡è‰²å’Œè™›ç·š)
        ax.plot(train_df['epoch'], train_df['train_loss_epoch'], marker=marker, markersize=3, linestyle='--', color=color, alpha=0.6, label=f'Train Loss ({name})', linewidth=1.5)
    
    ax.set_title('Learning Curve Comparison', fontsize=16, fontweight='bold')
    ax.set_xlabel('Epoch', fontsize=12)
    ax.set_ylabel('Loss (MSE) - Log Scale', fontsize=12)
    ax.set_yscale('log')
    ax.grid(True, which="both", ls="--", c='gray', alpha=0.5)
    
    '''
    # å»ºç«‹é›™Yè»¸ç”¨æ–¼ RÂ² Score
    ax2 = ax.twinx()
    
    for name, data in all_metrics.items():
        color = colors.get(name)
        marker = markers.get(name)
        df = data['df']
        val_r2_df = df[['epoch', 'val_r2']].dropna()
        
        if len(val_r2_df) > 0:
            # ä½¿ç”¨ä¸åŒçš„æ¨™è¨˜æ¨£å¼å€åˆ† RÂ² (é»ç‹€)
            ax2.plot(val_r2_df['epoch'], val_r2_df['val_r2'], marker=marker, markersize=4, linestyle=':', color=color, alpha=0.8, label=f'Val RÂ² ({name})', linewidth=2)
    
    ax2.set_ylabel('Validation RÂ² Score', fontsize=12, color='green')
    ax2.tick_params(axis='y', labelcolor='green')
    ax2.set_ylim([0, 1])
    
    '''
    # åˆä½µå…©å€‹è»¸çš„åœ–ä¾‹
    lines1, labels1 = ax.get_legend_handles_labels()
    # lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines1 , labels1 , loc='center left', fontsize='small')

def plot_final_performance(ax, all_metrics, colors):
    """åœ¨çµ¦å®šçš„ Matplotlib Axes ä¸Šç¹ªè£½æœ€çµ‚æ€§èƒ½é•·æ¢åœ– (Train Loss, Test Loss, RÂ² Score)ã€‚"""
    labels = list(all_metrics.keys())
    final_train_losses = [data['final_train_loss'] for data in all_metrics.values()]
    final_test_losses = [data['final_test_loss'] for data in all_metrics.values()]
    test_r2_scores = [data['test_r2'] for data in all_metrics.values()]
    
    x = np.arange(len(labels))
    width = 0.25
    
    # Create bars for each metric
    bars1 = ax.bar(x - width, final_train_losses, width, label='Train Loss', alpha=0.8, edgecolor='black')
    bars2 = ax.bar(x, final_test_losses, width, label='Test Loss', alpha=0.8, edgecolor='black')
    
    # Create secondary y-axis for RÂ² Score
    ax2 = ax.twinx()
    bars3 = ax2.bar(x + width, test_r2_scores, width, label='Test RÂ² Score', alpha=0.8, color='green', edgecolor='black')
    
    ax.set_title('Final Performance Comparison', fontsize=16, fontweight='bold')
    ax.set_ylabel('Loss (MSE)', fontsize=12)
    ax2.set_ylabel('RÂ² Score', fontsize=12)
    ax.set_xlabel('Version', fontsize=12)
    ax.set_xticks(x)
    ax.set_xticklabels(labels)
    ax.tick_params(axis='x', rotation=30)
    
    # åœ¨é•·æ¢åœ–ä¸Šæ¨™ç¤ºæ•¸å€¼
    for bar in bars1:
        height = bar.get_height()
        if not np.isnan(height):
            ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.6f}', ha='center', va='bottom', fontsize=9)
    
    for bar in bars2:
        height = bar.get_height()
        if not np.isnan(height):
            ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.6f}', ha='center', va='bottom', fontsize=9)
    
    for bar in bars3:
        height = bar.get_height()
        if not np.isnan(height):
            ax2.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.4f}', ha='center', va='bottom', fontsize=9)
    
    # Combine legends from both axes
    lines1, labels1 = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left', fontsize='small')

def generate_report_and_plots(all_metrics, save_dir):
    """ç”¢ç”Ÿæœ€çµ‚çš„é‡åŒ–å ±å‘Šèˆ‡æ•´åˆåœ–è¡¨ã€‚"""
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    # --- ç”¢ç”Ÿé‡åŒ–å ±å‘Š ---
    report_lines = []
    report_lines.append("\n" + "="*140)
    report_lines.append("ğŸ“Š Comprehensive Data Augmentation Benefit Analysis Report")
    report_lines.append("="*140)
    report_lines.append(f"{'Version':<15} | {'Train Loss':<18} | {'Test Loss':<18} | {'Test RÂ² Score':<18} | {'Generalization Gap':<20}")
    report_lines.append("-" * 140)
    
    for name, data in all_metrics.items():
        gap_str = f"{data['gap']:.2f}%" if not np.isnan(data['gap']) else "N/A"
        r2_str = f"{data['test_r2']:.4f}" if not np.isnan(data['test_r2']) else "N/A"
        report_lines.append(f"{name:<15} | {data['final_train_loss']:<18.6f} | {data['final_test_loss']:<18.6f} | {r2_str:<18} | {gap_str:<20}")
    report_lines.append("-" * 140)
    
    report_text = "\n".join(report_lines)
    
    # Print to terminal
    print(report_text)
    
    # Save to file
    report_file = save_path / "final_report.txt"
    with open(report_file, 'w') as f:
        f.write(report_text)
    print(f"\nâœ… Final report saved to: {report_file}")

    # --- ç¹ªè£½æ•´åˆåœ–è¡¨ ---
    fig, axes = plt.subplots(1, 2, figsize=(20, 8))
    fig.suptitle('Overall Experiment Comparison', fontsize=22, weight='bold')
    
    # å®šç¾©é¡è‰²å’Œæ¨™è¨˜ï¼Œç¢ºä¿ä¸€è‡´æ€§
    color_palette = sns.color_palette("viridis", len(all_metrics))
    colors = {name: color_palette[i] for i, name in enumerate(all_metrics.keys())}
    markers = ['o', 's', 'X', 'D', '^', 'v']
    marker_map = {name: markers[i % len(markers)] for i, name in enumerate(all_metrics.keys())}
    
    plot_learning_curves(axes[0], all_metrics, colors, marker_map)
    plot_final_performance(axes[1], all_metrics, colors)
    
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    # æª”åç¾åœ¨æ˜¯å›ºå®šçš„ï¼Œå› ç‚ºå®ƒæ˜¯ä¸€å¼µåŒ…å«æ‰€æœ‰çµæœçš„ç¸½åœ–
    plot_path = save_path / 'OVERALL_comparison_report.png'
    plt.savefig(plot_path, dpi=300)
    plt.close()
    
    print(f"\nâœ… Overall comparison plot saved to: {plot_path}")

def main():
    parser = argparse.ArgumentParser(description="Run and compare multiple data augmentation experiments.")
    parser.add_argument('--config', type=str, default='configs/train_config.yaml', help='Path to the configuration file.')
    # âœ… å…è¨±å¤šå€‹å¢é‡è³‡æ–™é›†è·¯å¾‘
    parser.add_argument('--datasets', nargs='+', required=True, help='List of dataset paths to compare. The first one is the baseline (e.g., 1x).')
    parser.add_argument('--skip_training', action='store_true', help='If set, skip training and only generate plots from existing logs.')
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    all_metrics = {}
    log_dir_root = Path("logs/csv") # å°‡æ—¥èªŒæ ¹ç›®éŒ„å®šç¾©åœ¨å¤–é¢
    
    if args.skip_training:
        # ã€åƒ…ç¹ªåœ–ã€‘
        print("Running in PLOT-ONLY mode. Skipping training...")
        
        # ç›´æ¥å¾ç¾æœ‰çš„æ—¥èªŒæª”æ¡ˆè®€å–æ•¸æ“š
        for data_path_str in args.datasets:
            data_path = Path(data_path_str)
            version_name = data_path.name.split('_')[-1]
            run_name = f"tactile_{version_name}"
            
            csv_path = log_dir_root / run_name / "version_0" / "metrics.csv"

            if not csv_path.exists():
                print(f"Warning: Log file not found for {version_name} at {csv_path}. Skipping.")
                continue
            
            print(f"Found log file for {version_name}: {csv_path}")
            df = pd.read_csv(csv_path)
            final_train_loss = df['train_loss_epoch'].dropna().iloc[-1]
            final_test_loss = df['test_loss'].dropna().iloc[-1] if 'test_loss' in df.columns and not df['test_loss'].dropna().empty else np.nan
            test_r2 = df['test_r2'].dropna().iloc[-1] if 'test_r2' in df.columns and not df['test_r2'].dropna().empty else np.nan
            gap = ((final_test_loss - final_train_loss) / final_train_loss * 100) if not np.isnan(final_test_loss) and final_train_loss != 0 else np.nan
            all_metrics[version_name] = {'df': df, 'final_train_loss': final_train_loss, 'final_test_loss': final_test_loss, 'test_r2': test_r2, 'gap': gap}
    
    # è¿´åœˆåŸ·è¡Œæ‰€æœ‰æŒ‡å®šçš„è³‡æ–™é›†
    else:
        for data_path_str in args.datasets:
            data_path = Path(data_path_str)
            # å¾è·¯å¾‘ä¸­è‡ªå‹•æå–ç‰ˆæœ¬åç¨± (ä¾‹å¦‚ '1x', '2x')
            version_name = data_path.name.split('_')[-1]
            
            # `train_single_model` çš„å¾Œç¶´ç¾åœ¨ä¹Ÿä½¿ç”¨ç‰ˆæœ¬åç¨±
            csv_path = train_single_model(config, data_path_str, 'tactile', version_name)
            
            # è®€å–çµæœä¸¦å„²å­˜
            df = pd.read_csv(csv_path)
            final_train_loss = df['train_loss_epoch'].dropna().iloc[-1]
            final_test_loss = df['test_loss'].dropna().iloc[-1] if 'test_loss' in df.columns and not df['test_loss'].dropna().empty else np.nan
            test_r2 = df['test_r2'].dropna().iloc[-1] if 'test_r2' in df.columns and not df['test_r2'].dropna().empty else np.nan
            gap = ((final_test_loss - final_train_loss) / final_train_loss * 100) if not np.isnan(final_test_loss) else np.nan
            
            all_metrics[version_name] = {
                'df': df,
                'final_train_loss': final_train_loss,
                'final_test_loss': final_test_loss,
                'test_r2': test_r2,
                'gap': gap
            }

    # æ‰€æœ‰è¨“ç·´éƒ½å®Œæˆå¾Œï¼Œç”¢ç”Ÿç¸½å ±å‘Šå’Œç¸½åœ–è¡¨
    if not all_metrics:
        print("No metrics available to generate report and plots.")
    else:
        generate_report_and_plots(all_metrics, './plots')
        
if __name__ == '__main__':
    main()